<paper 0>
# Query2doc: Query Expansion with Large Language Models 

Liang Wang and Nan Yang and Furu Wei<br>Microsoft Research<br>\{wangliang,nanya,fuwei\} @microsoft.com


#### Abstract

This paper introduces a simple yet effective query expansion approach, denoted as query $2 d o c$, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudodocuments. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by $3 \%$ to $15 \%$ on ad-hoc IR datasets, such as MSMARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.


## 1 Introduction

Information retrieval (IR) aims to locate relevant documents from a large corpus given a user issued query. It is a core component in modern search engines and researchers have invested for decades in this field. There are two mainstream paradigms for IR: lexical-based sparse retrieval, such as BM25, and embedding-based dense retrieval (Xiong et al., 2021; Qu et al., 2021). Although dense retrievers perform better when large amounts of labeled data are available (Karpukhin et al., 2020), BM25 remains competitive on out-ofdomain datasets (Thakur et al., 2021).

Query expansion (Rocchio, 1971; Lavrenko and Croft, 2001) is a long-standing technique that rewrites the query based on pseudo-relevance feedback or external knowledge sources such as WordNet. For sparse retrieval, it can help bridge the lexical gap between the query and the documents. However, query expansion methods like RM3 (Lavrenko and Croft, 2001; Lv and Zhai, 2009) have only shown limited success on popular datasets (Campos et al., 2016), and most state-ofthe-art dense retrievers do not adopt this technique. In the meantime, document expansion methods like doc2query (Nogueira et al., 2019) have proven to be effective for sparse retrieval.

In this paper, we demonstrate the effectiveness of LLMs (Brown et al., 2020) as query expansion models by generating pseudo-documents conditioned on few-shot prompts. Given that search queries are often short, ambiguous, or lack necessary background information, LLMs can provide relevant information to guide retrieval systems, as they memorize an enormous amount of knowledge and language patterns by pre-training on trillions of tokens.

Our proposed method, called query $2 d o c$, generates pseudo-documents by few-shot prompting LLMs and concatenates them with the original query to form a new query. This method is simple to implement and does not require any changes in training pipelines or model architectures, making it orthogonal to the progress in the field of LLMs and information retrieval. Future methods can easily build upon our query expansion framework.

For in-domain evaluation, we adopt the MSMARCO passage ranking (Campos et al., 2016), TREC DL 2019 and 2020 datasets. Pseudodocuments are generated by prompting an improved version of GPT-3 text-davinci-003 from OpenAI (Brown et al., 2020). Results show that query $2 d o c$ substantially improves the off-theshelf BM25 algorithm without fine-tuning any model, particularly for hard queries from the TREC DL track. Strong dense retrievers, including DPR (Karpukhin et al., 2020), SimLM (Wang et al., 2023), and E5 (Wang et al., 2022) also benefit from query $2 d o c$, although the gains tend to be diminishing when distilling from a strong cross-encoder based re-ranker. Experiments in zero-shot OOD settings demonstrate that our method outperforms strong baselines on most
datasets. Further analysis also reveals the importance of model scales: query2doc works best when combined with the most capable LLMs while small language models only provide marginal improvements over baselines. To aid reproduction, we release all the generations from text-davinci003 at https://huggingface.co/datasets/ intfloat/query2doc_msmarco.

## 2 Method

![](https://cdn.mathpix.com/cropped/2024_06_04_b64a5e17d3140b958f39g-02.jpg?height=748&width=594&top_left_y=817&top_left_x=320)

Figure 1: Illustration of query2doc few-shot prompting. We omit some in-context examples for space reasons.

Given a query $q$, we employ few-shot prompting to generate a pseudo-document $d^{\prime}$ as depicted in Figure 1. The prompt comprises a brief instruction "Write a passage that answers the given query:" and $k$ labeled pairs randomly sampled from a training set. We use $k=4$ throughout this paper. Subsequently, we rewrite $q$ to a new query $q^{+}$by concatenating with the pseudo-document $d^{\prime}$. There are slight differences in the concatenation operation for sparse and dense retrievers, which we elaborate on in the following section.

Sparse Retrieval Since the query $q$ is typically much shorter than pseudo-documents, to balance the relative weights of the query and the pseudodocument, we boost the query term weights by repeating the query $n$ times before concatenating with the pseudo-document $d^{\prime}$ :

$$
\begin{equation*}
q^{+}=\operatorname{concat}\left(\{q\} \times \mathrm{n}, d^{\prime}\right) \tag{1}
\end{equation*}
$$

Here, "concat" denotes the string concatenation function. $q^{+}$is used as the new query for BM25 retrieval. We find that $n=5$ is a generally good value and do not tune it on a dataset basis.

Dense Retrieval The new query $q^{+}$is a simple concatenation of the original query $q$ and the pseudo-document $d^{\prime}$ separated by [SEP]:

$$
\begin{equation*}
q^{+}=\operatorname{concat}\left(q,[\mathrm{SEP}], d^{\prime}\right) \tag{2}
\end{equation*}
$$

For training dense retrievers, several factors can influence the final performance, such as hard negative mining (Xiong et al., 2021), intermediate pretraining (Gao and Callan, 2021), and knowledge distillation from a cross-encoder based re-ranker (Qu et al., 2021). In this paper, we investigate two settings to gain a more comprehensive understanding of our method. The first setting is training DPR (Karpukhin et al., 2020) models initialized from $\mathrm{BERT}_{\text {base }}$ with BM25 hard negatives only. The optimization objective is a standard contrastive loss:

![](https://cdn.mathpix.com/cropped/2024_06_04_b64a5e17d3140b958f39g-02.jpg?height=130&width=601&top_left_y=1286&top_left_x=1139)

where $\mathbf{h}_{q}$ and $\mathbf{h}_{d}$ represent the embeddings for the query and document, respectively. $\mathbb{N}$ denotes the set of hard negatives.

The second setting is to build upon state-of-theart dense retrievers and use KL divergence to distill from a cross-encoder teacher model.

$$
\begin{equation*}
\min \quad D_{\mathrm{KL}}\left(p_{\mathrm{ce}}, p_{\mathrm{stu}}\right)+\alpha L_{\mathrm{cont}} \tag{4}
\end{equation*}
$$

$p_{\mathrm{ce}}$ and $p_{\text {stu }}$ are the probabilities from the crossencoder and our student model, respectively. $\alpha$ is a coefficient to balance the distillation loss and contrastive loss.

## Comparison with Pseudo-relevance Feedback

Our proposed method is related to the classic method of pseudo-relevance feedback (PRF) (Lavrenko and Croft, 2001; Lv and Zhai, 2009). In conventional PRF, the feedback signals for query expansion come from the top-k documents obtained in the initial retrieval step, while our method prompts LLMs to generate pseudo-documents. Our method does not rely on the quality of the initial retrieval results, which are often noisy or irrelevant. Rather, it exploits cutting-edge LLMs to generate documents that are more likely to contain relevant terms.

| Method | Fine-tuning | MS MARCO dev |  |  | TREC DL 19 <br> nDCG@ 10 | TREC DL 20 <br> nDCG@ 10 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | MRR@10 | R@50 | R@1k |  |  |
| Sparse retrieval |  |  |  |  |  |  |
| BM25 | $x$ | 18.4 | 58.5 | 85.7 | $51.2^{*}$ | $47.7^{*}$ |
| + query $2 \mathrm{doc}$ | $x$ | $21.4^{+3.0}$ | $65.3^{+6.8}$ | $91.8^{+6.1}$ | $66.2^{+15.0}$ | $\mathbf{6 2 . 9}^{+15.2}$ |
| BM25 + RM3 | $x$ | 15.8 | 56.7 | 86.4 | 52.2 | 47.4 |
| docT5query (Nogueira and Lin) | $\checkmark$ | $27.7 \quad$ | 75.6 | 94.7 | 64.2 | - |
| Dense retrieval w/o distillation |  |  |  |  |  |  |
| ANCE (Xiong et al., 2021) | $\checkmark$ | 33.0 | - | 95.9 | 64.5 | 64.6 |
| HyDE (Gao et al., 2022) | $x$ | - | - | - | 61.3 | 57.9 |
| DPR bert-base (our impl.) | $\checkmark$ | 33.7 | 80.5 | 95.9 | 64.7 | 64.1 |
| + query $2 \mathrm{doc}$ | $\checkmark$ | 35.1 ${ }^{1.4}$ | $\mathbf{8 2 . 6} \mathbf{6}^{2.1}$ | $97.2^{+1.3}$ | $68.7^{+4.0}$ | $67.1^{+3.0}$ |
| Dense retrieval w/ distillation |  |  |  |  |  |  |
| RocketQAv2 (Ren et al., 2021) | $\checkmark$ | 38.8 | 86.2 | 98.1 | - | - |
| AR2 (Zhang et al., 2022) | $\checkmark$ | 39.5 | 87.8 | 98.6 | - | - |
| SimLM (Wang et al., 2023) | $\checkmark$ | 41.1 | 87.8 | 98.7 | 71.4 | 69.7 |
| + query $2 \mathrm{doc}$ | $\checkmark$ | $41.5^{+0.4}$ | $88.0^{+0.2}$ | $\mathbf{9 8 . 8} \mathbf{8}^{0.1}$ | $72.9^{+1.5}$ | $71.6^{+1.9}$ |
| E5 base + KD (Wang et al., 2022) | $\checkmark$ | 40.7 | 87.6 | 98.6 | 74.3 | 70.7 |
| + query2doc | $\checkmark$ | $\mathbf{4 1 . 5}^{+0.8}$ | $\mathbf{8 8 . 1}^{\mathbf{+ 0 . 5}}$ | $98.7^{+0.1}$ | $74.9^{+0.6}$ | $\mathbf{7 2 . 5}^{+1.8}$ |

Table 1: Main results on the MS-MARCO passage ranking and TREC datasets. The "Fine-tuning" column indicates whether the method requires fine-tuning model on labeled data or not. *: our reproduction.

## 3 Experiments

### 3.1 Setup

Evaluation Datasets For in-domain evaluation, we utilize the MS-MARCO passage ranking (Campos et al., 2016), TREC DL 2019 (Craswell et al., 2020a) and 2020 (Craswell et al., 2020b) datasets. For zero-shot out-of-domain evaluation, we select five low-resource datasets from the BEIR benchmark (Thakur et al., 2021). The evaluation metrics include MRR@10, R@k $(\mathrm{k} \in\{50,1 \mathrm{k}\})$, and nDCG@ 10 .

Hyperparameters For sparse retrieval including BM25 and RM3, we adopt the default implementation from Pyserini (Lin et al., 2021). When training dense retrievers, we use mostly the same hyperparameters as SimLM (Wang et al., 2023), with the exception of increasing the maximum query length to 144 to include pseudo-documents. When prompting LLMs, we include 4 in-context examples and use the default temperature of 1 to sample at most 128 tokens. For further details, please refer to Appendix A.

### 3.2 Main Results

In Table 1 , we list the results on the MS-MARCO passage ranking and TREC DL datasets. For sparse retrieval, "BM25 + query2doc" beats the BM25 baseline with over $15 \%$ improvements on TREC DL 2019 and 2020 datasets. Our manual inspection reveals that most queries from the TREC DL track are long-tailed entity-centric queries, which benefit more from the exact lexical match. The traditional query expansion method RM3 only marginally improves the R@1k metric. Although the document expansion method docT5query achieves better numbers on the MS-MARCO dev set, it requires training a T5-based query generator with all the available labeled data, while "BM25 + query2doc" does not require any model fine-tuning.

For dense retrieval, the model variants that combine with query2doc also outperform the corresponding baselines on all metrics. However, the gain brought by query2doc tends to diminish when using intermediate pre-training or knowledge distillation from cross-encoder re-rankers, as shown by the "SimLM + query2doc" and "E5 + query2doc" results.

For zero-shot out-of-domain retrieval, the results are mixed as shown in Table 2. Entity-centric datasets like DBpedia see the largest improvements. On the NFCorpus and Scifact datasets, we observe a minor decrease in ranking quality. This is likely due to the distribution mismatch between training and evaluation.

## 4 Analysis

Scaling up LLMs is Critical For our proposed method, a question that naturally arises is: how does the model scale affect the quality of query expansion? Table 3 shows that the performance steadily improves as we go from the $1.3 \mathrm{~B}$ model

|  | DBpedia | NFCorpus | Scifact | Trec-Covid | Touche2020 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| BM25 | 31.3 | 32.5 | 66.5 | 65.6 | 36.7 |
| + query2doc | $37.0^{+5.7}$ | $34.9^{+2.4}$ | $68.6^{+2.1}$ | $72.2^{+6.6}$ | $\mathbf{3 9 . 8}^{+3.1}$ |
| SimLM (Wang et al., 2023) | 34.9 | 32.7 | 62.4 | 55.0 | 18.9 |
| + query2doc | $38.3^{+3.4}$ | $32.1^{-0.6}$ | $59.5^{-2.9}$ | $59.9^{+4.9}$ | $25.6^{+6.7}$ |
| E5 base + KD (Wang et al., 2022) | 40.7 | 35.0 | $\mathbf{7 0 . 4}^{-2}$ | 74.1 | 30.9 |
| + query2doc | $\mathbf{4 2 . 4 ^ { + 1 . 7 }}$ | $\mathbf{3 5 . 2}^{+0.2}$ | $67.5^{-2.9}$ | $\mathbf{7 5 . 1}^{+1.0}$ | $31.7^{+0.8}$ |

Table 2: Zero-shot out-of-domain results on 5 low-resource datasets from the BEIR benchmark (Thakur et al., 2021). The reported numbers are nDCG @ 10. For a fair comparison, the in-context examples for prompting LLMs come from the MS-MARCO training set.

|  | \# params | TREC 19 | TREC 20 |
| :--- | :---: | :---: | :---: |
| BM25 | - | 51.2 | 47.7 |
| w/ babbage | $1.3 \mathrm{~B}$ | 52.0 | 50.2 |
| w/ curie | 6.7B | 55.1 | 50.1 |
| w/ davinci-001 | 175B | 63.5 | 58.2 |
| w/ davinci-003 | 175B | 66.2 | 62.9 |
| w/ gpt-4 | - | $\mathbf{6 9 . 2}$ | $\mathbf{6 4 . 5}$ |

Table 3: Query expansion with different model sizes. Even though GPT-4 performs best, we are unable to apply it in the main experiments due to quota limits.

to 175B models. Empirically, the texts generated by smaller language models tend to be shorter and contain more factual errors. Also, the "davinci-003" model outperforms its earlier version "davinci-001" by using better training data and improved instruction tuning. The recently released GPT-4 (OpenAI, 2023) achieves the best results.

![](https://cdn.mathpix.com/cropped/2024_06_04_b64a5e17d3140b958f39g-04.jpg?height=520&width=671&top_left_y=1713&top_left_x=264)

Figure 2: MRR on MS-MARCO dev set w.r.t the percentage of labeled data used for fine-tuning.

## Performance Gains are Consistent across Data

Scales Figure 2 presents a comparison between two variants of DPR models, which differ in the amount of labeled data used. The results show that the "DPR + query 2 doc" variant consistently outperforms the DPR baseline by approximately $1 \%$, regardless of the amount of data used for finetuning. This observation highlights that our contribution is orthogonal to the continual scaling up of supervision signals.

|  | TREC 19 | TREC 20 |
| :---: | :---: | :---: |
| BM25 + query2doc | $\mathbf{6 6 . 2}$ | $\mathbf{6 2 . 9}$ |
| w/ query only | 51.2 | 47.7 |
| w/ pseudo-doc only | 48.7 | 44.5 |

Table 4: Using the concatenation of the original query and the generated pseudo-documents perform substantially better.

How to Use Pseudo-documents In this paper, we concatenate the original query and pseudodocuments as the new query. Alternatively, one can solely use the pseudo-documents, as done in the approach of HyDE (Gao et al., 2022). The results presented in Table 4 demonstrate that the original query and pseudo-documents are complementary, and their combination leads to substantially better performance in sparse retrieval.

Case Analysis In Table 5, we show two queries along with their respective pseudo-documents and groundtruth. The pseudo-documents, which are generated by LLMs, offer detailed and mostly accurate information, thereby reducing the lexical mismatch between the query and documents. In some cases, the pseudo-documents are sufficient to meet the user's information needs, rendering the retrieval step unnecessary. However, it is worth noting that the LLM generations may contain factual errors. For instance, in the second query, the theme song "It's a Jungle Out There" was used as of season two in 2003 , not $2002^{1}$. Although such errors may appear subtle and difficult to verify, they pose a significant challenge to building trustworthy systems using LLMs.[^0]

| query | who killed nicholas ii of russia |
| :--- | :--- |
| LLM generation | Nicholas II of Russia, the last Tsar of Russia, was executed by Bolshevik revolutionaries on <br> July 17th, 1918, along with his wife Alexandra and their five children. The execution was <br> ordered by Vladimir Lenin, the leader of the Bolshevik Party. The actual execution was carried <br> out by a firing squad of Bolshevik troops, and the bodies were later buried in a mass grave. . |
| Groundtruth | (1868-1918). Nicholas II was the last emperor, or tsar, of Russia, serving from 1894 to <br> 1917. Nicholas, his wife, and their five children were killed by the Bolsheviks, members of a <br> revolutionary group (led by Lenin) who seized control of the government in Russia during the <br> October Revolution (see Russian Revolution) of 1917. |
| query | who sings monk theme song |
| LLM generation | The theme song for the television show Monk is entitled "It's a Jungle Out There" and is <br> sung by American singer-songwriter Randy Newman. The song was written specifically for <br> the show, and it has been used as the theme song since the series premiered in 2002. It has <br> been praised by critics and fans alike and is often regarded as one of the best theme songs in <br> television history. |
| Groundtruth | exists and is an alternate of. The Monk theme song is It's a Jungle Out There by Randy <br> Newman. The Monk theme song is It's a Jungle Out There by Randy Newman. |

Table 5: Examples from the TREC DL 2020 dataset. Bold texts are the overlapping words between groundtruth and pseudo-documents generated from LLMs. The italicized red sentence demonstrates a factual error in language model generations.

## 5 Related Work

Query Expansion and Document Expansion are two classical techniques to improve retrieval quality, particularly for sparse retrieval systems. Both techniques aim to minimize the lexical gap between the query and the documents. Query expansion typically involves rewriting the query based on relevance feedback (Lavrenko and Croft, 2001; Rocchio, 1971) or lexical resources such as WordNet (Miller, 1992). In cases where labels are not available, the top-k retrieved documents can serve as pseudo-relevance feedback signals (Lv and Zhai, 2009). Liu et al. fine-tunes an encoder-decoder model to generate contextual clues.

In contrast, document expansion enriches the document representation by appending additional relevant terms. Doc2query (Nogueira et al., 2019) trains a seq2seq model to predict pseudo-queries based on documents and then adds generated pseudo-queries to the document index. Learned sparse retrieval models such as SPLADE (Formal et al., 2021) and uniCOIL (Lin and Ma, 2021) also learn document term weighting in an end-to-end fashion. However, most state-of-the-art dense retrievers (Ren et al., 2021; Wang et al., 2023) do not adopt any expansion techniques. Our paper demonstrates that strong dense retrievers also benefit from query expansion using LLMs.

Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and LLaMA (Touvron et al., 2023) are trained on trillions of tokens with billions of param- eters, exhibiting unparalleled generalization ability across various tasks. LLMs can follow instructions in a zero-shot manner or conduct in-context learning through few-shot prompting. Labeling a few high-quality examples only requires minimal human effort. In this paper, we employ few-shot prompting to generate pseudo-documents from a given query. A closely related recent work HyDE (Gao et al., 2022) instead focuses on the zeroshot setting and uses embeddings of the pseudodocuments for similarity search. HyDE implicitly assumes that the groundtruth document and pseudodocuments express the same semantics in different words, which may not hold for some queries. In the field of question answering, RECITE (Sun et al., 2022) and GENREAD (Yu et al., 2022) demonstrate that LLMs are powerful context generators and can encode abundant factual knowledge. However, as our analysis shows, LLMs can sometimes generate false claims, hindering their practical application in critical areas.

## 6 Conclusion

This paper presents a simple method query $2 d o c$ to leverage LLMs for query expansion. It first prompts LLMs with few-shot examples to generate pseudo-documents and then integrates with existing sparse or dense retrievers by augmenting queries with generated pseudo-documents. The underlying motivation is to distill the LLMs through prompting. Despite its simplicity, empirical evaluations demonstrate consistent improvements across various retrieval models and datasets.

## Limitations

|  | LLM call | Index search |
| :---: | :---: | :---: |
| BM25 | - | $16 \mathrm{~ms}$ |
| + query2doc | $>2000 \mathrm{~ms}$ | $177 \mathrm{~ms}$ |

Table 6: Latency analysis for retrieval systems with our proposed query2doc. We retrieve the top 100 results for MS-MARCO dev queries with a single thread and then average over all the queries. The latency for LLM API calls depends on server load and is difficult to precisely measure.

An apparent limitation is the efficiency of retrieval. Our method requires running inference with LLMs which can be considerably slower due to the token-by-token autoregressive decoding. Moreover, with query2doc, searching the inverted index also becomes slower as the number of query terms increases after expansion. This is supported by the benchmarking results in Table 6. Real-world deployment of our method should take these factors into consideration.

## References

Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al. 2022. Overview of touché 2022: argument retrieval. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 311-336. Springer.

Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A full-text learning to rank dataset for medical information retrieval. In European Conference on Information Retrieval, pages $716-722$. Springer.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms marco: A human generated machine reading comprehension dataset. ArXiv preprint, abs/1611.09268.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311.

Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020a. Overview of the trec 2019 deep learning track. ArXiv preprint, abs/2003.07820.

Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv preprint, abs/2003.07820.

Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. Splade: Sparse lexical and expansion model for first stage ranking. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.

Luyu Gao and Jamie Callan. 2021. Condenser: a pretraining architecture for dense retrieval. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 981-993, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise zero-shot dense retrieval without relevance labels. ArXiv preprint, abs/2212.10496.

Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. Dbpedia-entity v2: A test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, pages $1265-1268$. ACM.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and

Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.

Victor Lavrenko and W. Bruce Croft. 2001. Relevancebased language models. ACM SIGIR Forum, 51:260 -267 .

Jimmy J. Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. ArXiv preprint, abs/2106.14807.

Jimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, Rodrigo Nogueira, and David R. Cheriton. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.

Linqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel, and Pontus Stenetorp. 2022. Query expansion using contextual clue sampling with language models. ArXiv preprint, abs/2210.07093.

Yuanhua Lv and ChengXiang Zhai. 2009. A comparative study of methods for estimating query language models with pseudo feedback. Proceedings of the 18th ACM conference on Information and knowledge management.

George A. Miller. 1992. WordNet: A lexical database for English. In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992.

Rodrigo Nogueira and Jimmy Lin. From doc2query to docttttquery.

Rodrigo Nogueira, Wei Yang, Jimmy J. Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. ArXiv preprint, abs/1904.08375.

OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.

Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.
Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

J. J. Rocchio. 1971. Relevance feedback in information retrieval.

Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. ArXiv preprint, abs/2210.01296.

Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971.

Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. Trec-covid: constructing a pandemic information retrieval test collection. In ACM SIGIR Forum, volume 54, pages 1-12. ACM New York, NY, USA.

David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534-7550, Online. Association for Computational Linguistics.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. ArXiv preprint, $\mathrm{abs} / 2212.03533$.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2023. SimLM: Pre-training with representation bottleneck for dense passage retrieval. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2244-2258, Toronto, Canada. Association for Computational Linguistics.

Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning

Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

W. Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators. ArXiv preprint, abs/2209.10063.

Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2022. Adversarial retriever-ranker for dense text retrieval. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.
</end of paper 0>


<paper 1>
# Query Expansion by Prompting Large Language Models 

Rolf Jagerman<br>Google Research<br>jagerman@google.com

Honglei Zhuang<br>Google Research<br>hlz@google.com

Zhen Qin<br>Google Research<br>zhenqin@google.com

Xuanhui Wang<br>Google Research<br>xuanhui@google.com

Michael Bendersky<br>Google Research<br>bemike@google.com


#### Abstract

Query expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.


## 1 INTRODUCTION

Query expansion is a widely used technique that improves the recall of search systems by adding additional terms to the original query. The expanded query may be able to recover relevant documents that had no lexical overlap with the original query. Traditional query expansion approaches are typically based on Pseudo-Relevance Feedback (PRF) [1, 20, 21, 23], which treats the set of retrieved documents from the original query as "pseudo-relevant" and uses those documents' contents to extract new query terms. However, PRF-based approaches assume that the top retrieved documents are relevant to the query. In practice the initial retrieved documents may not be perfectly aligned with the original query, especially if the query is short or ambiguous. As a result, PRF-based approaches may fail if the initial set of retrieved documents is not good enough

In this paper we propose the use of Large Language Models (LLMs) $[3,8,19]$ to aid in query expansion. LLMs have seen a growing interest in the Information Retrieval (IR) community in recent years. They exhibit several properties, including the ability to answer questions and generate text, that make them powerful tools. We propose using those generative abilities to generate useful query expansions. In particular we investigate ways to prompt an LLM and have it generate a variety of alternative and new terms for the original query. This means that, instead of relying on the knowledge within PRF documents or lexical knowledge bases, we rely on the knowledge inherent in the LLM. An example of the proposed methodology is presented in Figure 1.

Our main contributions in this work are as follows: First, we formulate various prompts to perform query expansion (zero-shot,

![](https://cdn.mathpix.com/cropped/2024_06_04_3f10aa351a6c66d8fdbfg-1.jpg?height=542&width=566&top_left_y=691&top_left_x=1235)

Figure 1: High-level overview of using a zero-shot Chain-ofThought (CoT) prompt to generate query expansion terms.

few-shot and CoT) with and without PRF to study their relative performance. Second, we find that Chain-of-Thought (CoT) prompts perform best and hypothesize that this is because CoT prompts instruct the model to break its answer down step-by-step which includes many keywords that can aid in query expansion. Finally, we study the performance across various model sizes to better understand the practical capabilities and limitations of an LLM approach to query expansion.

## 2 RELATED WORK

Query expansion is widely studied [4, 11]. At its core, query expansion helps retrieval systems by expanding query terms into new terms that express the same concept or information need, increasing the likelihood of a lexical match with documents in the corpus. Early works on query expansion focused on either using lexical knowledge bases [2, 18, 29] or Pseudo-Relevance Feedback (PRF) $[1,20,23]$. PRF-based approaches are particularly useful in practice because they do not need to construct a domain-specific knowledge base and can be applied to any corpus. Orthogonal to query expansion is document expansion [10, 16, 25, 33] which applies similar techniques but expands document terms during indexing instead of query terms during retrieval.

Recent works on query expansion have leveraged neural networks to generate or select expansion terms $[13,24,33,34]$, generally by either training or fine-tuning a model. In contrast, our work leverages the abilities inherent in general-purpose LLMs without needing to train or fine-tune the model.

We note that our work is similar to the recent works of [7] and [31]: leveraging an LLM to expand a query. However, we differentiate our work in several important ways: First, we study a number of different prompts whereas [31] focuses on a single few-shot prompt and [7] does not study prompts. Second, unlike [31] and [7], we focus on generating query expansion terms instead of entire pseudo documents. To this end, we demonstrate the performance of our prompts on a variety of smaller model sizes which helps understand both the limitations and the practical capabilities of an LLM approach to query expansion. Finally, we experiment with entirely open-source models, inviting reproducibility and openness of research, while [31] experiments with a single type of model which is only accessible through a third-party API.

## 3 METHODOLOGY

We formulate the query expansion problem as follows: given a query $q$ we wish to generate an expanded query $q^{\prime}$ that contains additional query terms that may help in retrieving relevant documents. In particular we study the use of an LLM to expand the query terms and generate a new query $q^{\prime}$. Since the LLM output may be verbose, we repeat the original query terms 5 times to upweigh their relative importance. This is the same as the trick employed by [31]. More formally:

$$
\begin{equation*}
q^{\prime}=\operatorname{Concat}\left(q, q, q, q, q, \operatorname{LLM}\left(\text { prompt }_{q}\right)\right) \tag{1}
\end{equation*}
$$

where Concat is the string concatenation operator, $q$ is the original query, LLM is a Large Language Model and prompt ${ }_{q}$ is the generated prompt based on the query (and potentially side information like few-shot examples or PRF documents).

In this paper we study eight different prompts:

Q2D The Query2Doc [31] few-shot prompt, asking the model to write a passage that answers the query.

Q2D/ZS A zero-shot version of Q2D.

Q2D/PRF A zero-shot prompt like Q2D/ZS but which also contains extra context in the form of top-3 retrieved PRF documents for the query.

Q2E Similar to the Query2Doc few-shot prompt but with examples of query expansion terms instead of documents.

Q2E/ZS A zero-shot version of Q2E.

Q2E/PRF A zero-shot prompt like $\mathbf{Q 2 E} / \mathbf{Z S}$ but with extra context in the form of PRF documents like Q2D/PRF.

CoT A zero-shot Chain-of-Thought prompt which instructs the model to provide rationale for its answer.

CoT/PRF A prompt like CoT but which also contains extra context in the form of top-3 retrieved PRF documents for the query.

Zero-shot prompts (Q2D/ZS and $\mathbf{Q 2 E} / \mathbf{Z S}$ ) are the simplest as they consist of a simple plaintext instruction and the input query. Fewshot prompts (Q2D and Q2E) additionally contain several examples to support in-context learning, for example they contain queries and corresponding expansions. Chain-of-Thought (CoT) prompts formulate their instruction to obtain a more verbose output from the model by asking it to break its response down step-by-step. Finally, Pseudo-Relevance Feedback (./PRF) variations of prompts use the top-3 retrieved documents as additional context for the model. See Appendix A for the exact prompts that are used in the experiments.

## 4 EXPERIMENTS

To validate the effectiveness of the LLM-based query expansion we run experiments on two retrieval tasks: MS-MARCO [15] passage retrieval and BEIR [27]. For the retrieval system we use BM25 [21, 22] as implemented by Terrier [17] ${ }^{1}$. We use the default BM25 parameters $\left(b=0.75, k_{1}=1.2, k_{3}=8.0\right)$ provided by Terrier.

### 4.1 Baselines

To analyze the LLM-based query expansion methods we compare against several classical PRF-based query expansion methods [1]:

- Bo1: Bose-Einstein 1 weighting
- Bo2: Bose-Einstein 2 weighting
- KL: Kullback-Leibler weighting

The implementations for these are provided by Terrier. In all cases we use the default Terrier settings for query expansion: 3 PRF docs and 10 expansion terms.

Furthermore, we include the prompt from Query2Doc [31] as a baseline. However, we do not compare against their exact setup since they use a significantly larger model than the models we study in this paper. The comparisons in this paper are focused on prompts and not on the exact numbers produced by different, potentially much larger, models. Furthermore, for models with a small receptive field (specifically the Flan-T5 models) we only use a 3-shot Q2D prompt instead of the standard 4-shot prompt to prevent the prompt from being truncated.

### 4.2 Language Models

We compare the prompts on two types of models, Flan-T5 [6,19] and Flan-UL2 [26], at various model sizes:

- Flan-T5-Small (60M parameters)
- Flan-T5-Base (220M parameters)
- Flan-T5-Large (770M parameters)
- Flan-T5-XL (3B parameters)
- Flan-T5-XXL (11B parameters)
- Flan-UL2 (20B parameters)

We choose to use the Flan [6,32] versions of the T5 [19] and UL2 [26] models as they are fine-tuned to follow instructions which is critical when using prompt-based approaches. Furthermore, all of these models are available as open-source ${ }^{2}$.

### 4.3 Metrics

Since we are interested in query expansion, which is largely focussed on improving the recall of first-stage retrieval, we use Recall $@ 1 \mathrm{~K}$ as our core evaluation metric. We also report top-heavy ranking metrics using MRR@10 [30] and NDCG@10 [14] to better understand how the models change the top retrieved results. We report all our results with significance testing using a paired $t$-test and consider a result significant at $p<0.01$.[^0]

## 5 RESULTS

### 5.1 MS-MARCO Passage Ranking

Table 1 presents the results on the MS-MARCO passage ranking task. The classical query expansion baselines (Bo1, Bo2 and KL), already provide a useful gain in terms of Recall@1K over the standard BM25 retrieval. In line with the results of [12], we observe that this increase in recall comes at the cost of top-heavy ranking metrics such as MRR@10 and NDCG@10.

Next, we see the results of LLM-based query expansion depend heavily on the type of prompts used. Similar to the findings of [31], the Query2Doc prompt (Q2D) can provide a substantial gain in terms of Recall@1K over the classical approaches. Interestingly, Query2Doc does not only improve recall, but also improves the topheavy ranking metrics such as MRR@10 and NDCG@10, providing a good improvement across metrics. This contrasts with classical query expansion methods which typically sacrifice top-heavy ranking metrics in order to improve recall.

Finally, the best performance is obtained by CoT (and the corresponding PRF-enhanced prompt CoT/PRF). This particular prompt instructs the model to generate a verbose explanation by breaking its answer down into steps. We hypothesize that this verbosity may lead to many potential keywords that are useful for query expansion. Finally, we find that adding PRF documents to the prompt helps significantly in top-heavy ranking metrics like MRR@10 and NDCG@10 across models and prompts. A possible explanation for this is that LLMs are effective in distilling the PRF documents, which may already contain relevant passages, by attending over the most promising keywords and using them in the output. We provide a more concrete example of the prompt output in Appendix B.

### 5.2 BEIR

The BEIR datasets comprise many different zero-shot information retrieval tasks from a variety of domains. We compare the performance of the different prompts on the BEIR datasets in Table 2. The first thing to observe here is that the classical PRF-based query expansion baselines still work very well, especially on domainspecific datasets such as trec-covid, scidocs and touche2020. These datasets are largely academic and scientific in nature, and the PRF documents may provide useful query terms in these cases. In contrast, the general purpose LLMs may not have sufficient domain knowledge to be useful for these datasets. Second, we note that the question-answering style datasets (fiqa, hotpotqa, msmarco and nq) seem to benefit the most from an LLM approach to query expansion. It is likely that the language model is producing relevant answers towards the query which helps retrieve the relevant passages more effectively. Across all datasets, the Q2D/PRF prompt produces the highest average Recall@1K, with the CoT prompt as a close second.

### 5.3 The Impact of Model Size

To understand the practical capabilities and limitations of an LLMbased query expander, we compare different model sizes in Figure 2. We range the model size from $60 \mathrm{M}$ parameters (Flan-T5-small) up to 11B (Flan-T5-XXL) and also try a 20B parameter model (Flan-UL2) but note that the latter also has a different pre-training objective. In general we observe the expected trend that larger models tend to
Table 1: LLM-based query expansion on the MS-MARCO passage ranking dev set. $\triangle$ indicates a statistically significant (paired $t$-test, $p<0.01$ ) improvement relative to the Q2D Flan-UL2 method. The best result per metric is bolded.

|  | Recall@1K | MRR@10 | NDCG@10 |
| :--- | ---: | ---: | ---: |
| BM25 | 87.82 | 18.77 | 23.44 |
| BM25 + Bo1 | 88.68 | 17.75 | 22.48 |
| BM25 + Bo2 | 88.32 | 17.58 | 22.30 |
| BM25 + KL | 88.62 | 17.71 | 22.44 |
| Flan-T5-XXL (11B) |  |  |  |
| Q2D | 88.76 | 19.07 | 23.76 |
| Q2D/ZS | 88.88 | 18.55 | 23.13 |
| Q2D/PRF | 89.31 | $22.13^{\mathbf{\Delta}}$ | $26.43^{\mathbf{\Delta}}$ |
| Q2E | 87.74 | 18.74 | 23.37 |
| Q2E/ZS | 87.93 | 18.79 | 23.45 |
| Q2E/PRF | 88.20 | 19.20 | 23.83 |
| CoT | 89.86 | 19.16 | 23.82 |
| CoT/PRF | 89.02 | $22.08^{\mathbf{\Delta}}$ | $26.32^{\mathbf{\Delta}}$ |
| Flan-UL2 (20B) |  |  |  |
| Q2D | 89.87 | 19.22 | 23.96 |
| Q2D/ZS | 86.60 | 15.56 | 19.54 |
| Q2D/PRF | 89.28 | $21.42^{\mathbf{\Delta}}$ | $25.82^{\mathbf{\Delta}}$ |
| Q2E | 88.04 | 18.84 | 23.52 |
| Q2E/ZS | 88.11 | 18.87 | 23.56 |
| Q2E/PRF | 88.43 | 19.24 | 23.90 |
| CoT | $\mathbf{9 0 . 6 1}$ | $20.05 \mathbf{\Delta}$ | $24.85^{\mathbf{\Delta}}$ |
| CoT/PRF | 89.30 | $\mathbf{2 2 . 6 2}$ | $\mathbf{2 6 . 8 9}$ |

![](https://cdn.mathpix.com/cropped/2024_06_04_3f10aa351a6c66d8fdbfg-3.jpg?height=558&width=794&top_left_y=1396&top_left_x=1121)

Figure 2: Performance on MS-MARCO passage ranking dev set across different model sizes. The shaded areas indicate a $\mathbf{9 9 \%}$ confidence interval.

perform better. The Q2D approach requires at least an 11B parameter model to reach parity with the BM25+Bo1 baseline. In contrast, the CoT approach only needs a 3B parameter model to reach parity. Furthermore, adding PRF documents to the CoT prompt seems to help stabilize the performance for smaller model sizes but does inhibit its performance at larger capacities. A possible explanation for this behavior is that the PRF documents decreases the creativity of

Table 2: Recall@1K of various prompts on BEIR using Flan-UL2. ${ }^{\Delta}$ indicates a statistically significant (paired $t$-test, $p<0.01$ ) improvement relative to the best classical QE method. The best result per dataset is highlighted in bold.

| Dataset | BM25 | Classical QE |  |  | LLM-based QE |  |  |  |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Bo1 | Bo2 | $\mathrm{KL}$ | Q2D | Q2D/ZS | $\mathrm{Q} 2 \mathrm{D} / \mathrm{PRF}$ | Q2E | Q2E/ZS | Q2E/PRF | $\mathrm{CoT}$ | CoT/PRF |
| arguana | 98.93 | 99.00 | 99.00 | 99.00 | 98.86 | 98.93 | 98.93 | 98.93 | 98.93 | 98.93 | 98.93 | 98.86 |
| climate-fever | 46.60 | 45.69 | 45.38 | 45.65 | 47.62 | 47.66 | 47.94 | 46.08 | 46.44 | 46.44 | 47.42 | 46.81 |
| cqadupstack | 65.55 | 66.82 | 66.57 | 66.70 | 65.51 | 64.19 | 65.01 | 65.69 | 65.71 | 65.90 | 66.39 | 66.12 |
| dbpedia | 63.72 | 64.77 | 64.55 | 64.60 | 65.89 | 65.47 | 65.78 | 63.55 | 63.92 | 63.93 | 65.77 | 65.06 |
| fever | 75.73 | 76.28 | 75.83 | 76.32 | $79.06^{4}$ | $78.87^{\star}$ | 77.29 | 75.78 | 75.79 | 76.27 | $78.21^{\Delta}$ | 77.25 |
| fiqa | 77.42 | 79.18 | 79.06 | 78.84 | 78.34 | 78.26 | 78.69 | 77.33 | 77.31 | 77.68 | 80.08 | 79.03 |
| hotpotqa | 85.78 | 84.84 | 81.71 | 84.65 | $86.90^{\Delta}$ | 85.71 | $87.58^{\triangle}$ | 85.60 | 85.54 | $87.25^{\star}$ | $87.54^{\boldsymbol{4}}$ | $88.79^{\triangle}$ |
| msmarco | 73.61 | 75.08 | 75.14 | 74.66 | 76.77 | 75.73 | 78.75 | 73.87 | 73.79 | 74.14 | 79.58 | 78.36 |
| nfcorpus | 38.70 | 57.30 | 57.67 | 56.46 | 55.34 | 59.81 | 59.68 | 43.38 | 44.12 | 47.06 | 52.63 | 53.32 |
| nq | 78.96 | 81.09 | 80.64 | 80.82 | $85.18^{\Delta}$ | $84.71^{\Delta}$ | $83.53^{\triangle}$ | 79.30 | 79.11 | 80.35 | $85.46^{\triangle}$ | $83.11^{\Delta}$ |
| quora | 99.26 | 99.20 | 99.12 | 99.20 | 99.00 | 98.84 | 98.92 | 99.25 | 99.29 | 99.26 | 99.17 | 99.21 |
| scidocs | 57.46 | 59.78 | 61.03 | 59.86 | 59.09 | 59.78 | 60.10 | 57.88 | 57.70 | 58.32 | 58.51 | 59.69 |
| scifact | 97.17 | 97.57 | 97.57 | 97.57 | 97.57 | 97.57 | 97.57 | 97.17 | 97.17 | 97.17 | 97.57 | 97.17 |
| touche2020 | 84.96 | 85.94 | 86.38 | 86.01 | 83.61 | 83.44 | 84.54 | 85.21 | 85.02 | 86.04 | 85.51 | 84.58 |
| trec-covid | 42.58 | 45.21 | 45.58 | 45.39 | 43.52 | 38.05 | 44.17 | 43.16 | 43.12 | 43.85 | 43.43 | 44.02 |
| Average | 72.43 | 74.52 | 74.35 | 74.38 | 74.82 | 74.47 | 75.23 | 72.81 | 72.86 | 73.50 | 75.08 | 74.76 |

the model, as it may focus too much on the provided documents. Although this helps prevent the model from making errors at smaller model sizes, it also inhibits the creative abilities that we wish to leverage at larger model sizes. The CoT/PRF prompt is able to outperform the other prompts at the $770 \mathrm{M}$ parameter model size, making it a good candidate for possible deployment in realistic search settings where serving a larger model may be impossible. Overall, it is clear that large models are able to provide significant gains which may limit the practical application of an LLM approach to query expansion. Distillation has been shown to be an effective way to transfer the ability of a large model to a smaller one. We leave the study of distillation of these models for query expansion as future work.

## 6 LIMITATIONS \& FUTURE WORK

There are a number of limitations in our work: First, we only study sparse retrieval (BM25) which is where query expansion is important. Dense retrieval systems (e.g. dual encoders) are less prone to the vocabulary gap and, as a result, are less likely to benefit from a query expansion. Wang et al. [31] has already studied this setting in more detail and we leave the analysis of our prompts for a dense retrieval setting as future work. Second, our work focuses on Flan [32] instruction-finetuned language models. We chose these models due to their ability to follow instructions and the fact that these models are open-source. Our work can naturally be extended to other language models $[3,5,9,28]$ and we leave the study of such models as a topic for future research. Third, we study specific prompt templates (see Appendix A) and there may be other ways to formulate the different prompts. Finally, the computational cost of LLMs may be prohibitive to deploy LLM-based query expansions in practice. It may be possible to distill the output of the large model into a smaller servable model. How to productionize LLM-based query expansions is left as an open problem.

## 7 CONCLUSION

In this paper we study LLM-based query expansions. In contrast to traditional PRF-based query expansion, LLMs are not restricted to the initial retrieved set of documents and may be able to generate expansion terms not covered by traditional methods. Our proposed method is simple: we prompt a large language model and provide it a query, then we use the model's output to expand the original query with new terms that help during document retrieval.

Our results show that Chain-of-Thought prompts are especially promising for query expansion, since they instruct the model to generate verbose explanations that can cover a wide variety of new keywords. Furthermore, our results indicate that including PRF documents in various prompts can improve top-heavy ranking metric performance during the retrieval stage and is more robust when used with smaller model sizes, which can help practical deployment of LLM-based query expansion.

As demonstrated in this paper, IR tasks like query expansion can benefit from LLMs. As the capabilities of LLMs continue to improve, it is promising to see their capabilities translate to various IR tasks. Furthermore, as LLMs become more widely available, they will be easier to use and deploy as core parts of IR systems which is exciting for both practitioners and researchers of such systems.

## REFERENCES

[1] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. $A C M$ Transactions on Information Systems (TOIS) 20, 4 (2002), 357-389.

[2] Jagdev Bhogal, Andrew MacFarlane, and Peter Smith. 2007. A review of ontology based query expansion. Information processing \& management 43, 4 (2007), 866886 .

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.

[4] Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. Acm Computing Surveys (CSUR) 44, 1 (2012), $1-50$.

[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).

[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).

[7] Vincent Claveau. 2021. Neural text generation for query expansion in information retrieval. In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology. 202-209.

[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[9] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning. PMLR, 5547-5569.

[10] Miles Efron, Peter Organisciak, and Katrina Fenlon. 2012. Improving retrieval of short texts through document expansion. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. 911920 .

[11] Efthimis N Efthimiadis. 1996. Query Expansion. Annual review of information science and technology (ARIST) 31 (1996), 121-87.

[12] D Harman. [n.d.]. Relevance feedback and other query modification techniques. Information Retrieval: Data Structures \& Algorithms ([n. d.]), 241-263.

[13] Ayyoob Imani, Amir Vakili, Ali Montazer, and Azadeh Shakery. 2019. Deep neural networks for query expansion using word embeddings. In Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14-18, 2019, Proceedings, Part II 41. Springer, 203-210.

[14] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), $422-446$.

[15] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. choice 2640 (2016), 660 .

[16] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).

[17] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Douglas Johnson. 2005. Terrier information retrieval platform. In Advances in Information Retrieval: 27th European Conference on IR Research, ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005. Proceedings 27. Springer, 517-519.

[18] Yonggang Qiu and Hans-Peter Frei. 1993. Concept based query expansion. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval. 160-169.

[19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The fournal of Machine Learning Research 21, 1 (2020), 5485-5551.

[20] Stephen E Robertson. 1990. On term selection for query expansion. Journal of documentation 46, 4 (1990), 359-364.

[21] Stephen E Robertson and K Sparck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information science 27, 3 (1976), 129146.

[22] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995), 109 .

[23] Joseph John Rocchio Jr. 1971. Relevance feedback in information retrieval. The SMART retrieval system: experiments in automatic document processing (1971).

[24] Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, and Utpal Garain. 2016. Using word embeddings for automatic query expansion. arXiv preprint arXiv:1606.07608 (2016).

[25] Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 2006. Language model information retrieval with document expansion. In Proceedings of the
Human Language Technology Conference of the NAACL, Main Conference. 407414 .

[26] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131 (2022).

[27] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview. net/forum?id=wCu6T5xFjeJ

[28] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).

[29] Ellen M Voorhees. 1994. Query expansion using lexical-semantic relations. In SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 61-69.

[30] Ellen M Voorhees et al. 1999. The trec-8 question answering track report.. In Trec, Vol. 99. 77-82.

[31] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. arXiv preprint arXiv:2303.07678 (2023)

[32] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021)

[33] Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. BERT$\mathrm{QE}$ : contextualized query expansion for document re-ranking. arXiv preprint arXiv:2009.07258 (2020).

[34] Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2021. Contextualized query expansion via unsupervised chunk selection for text retrieval. Information Processing \& Management 58, 5 (2021), 102672.
</end of paper 1>


<paper 2>
# LGDE: Local Graph-based Dictionary Expansion 

Dominik J. Schindler ${ }^{1, *} \quad$ Sneha Jha $^{1}$ Xixuan Zhang ${ }^{2,3}$<br>Kilian Buehling ${ }^{2,3}$ Annett Heft ${ }^{2,3}$ Mauricio Barahona ${ }^{1}$<br>${ }^{1}$ Imperial College London, UK<br>${ }^{2}$ Weizenbaum Institute Berlin, Germany<br>${ }^{3}$ Free University of Berlin, Germany<br>* Corresponding author: dominik.schindler19@imperial.ac.uk


#### Abstract

Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection. Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary. At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of predefined seed keywords. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association. We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities. We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary.


## 1 Introduction

Dictionary expansion aims to expand a set of preselected keywords by adding related terms that can enhance original queries in keyword-based information retrieval tasks. Designing a dictionary without in-depth knowledge of the vocabulary in the domain of interest is prone to inaccurate, nonspecific or incomplete results. Therefore, expertgenerated seed dictionaries are typically expanded with domain-specific keywords for diverse applications, such as patent searches (Lee et al., 2014), queries of bibliometric databases (Yin et al., 2020) and online forums (Gharibshah et al., 2022), query expansion for more effective web searches (Roy et al., 2016; Kuzi et al., 2016), or collecting topic-specific content from social media platforms (Bruns et al., 2020; Zeng and Schäfer, 2021; Klinger et al., 2022; van Atteveldt et al., 2022). Dictionary expansion is particularly relevant, and challenging, in domains with evolving semantics where word choice and language style are highly specialised and diverge from general language usage or are constantly in flux to ensure exclusive, community-internal communication, to adjust to ongoing events or the emergence of topics and cultural changes, or to avoid legal prosecution (Heft et al., 2023).

When retrieving information around a certain topic, the challenge becomes to find a 'good' dictionary that leads to a corpus containing most documents associated with the topic (high recall) and with few irrelevant documents (high precision). New approaches for data-driven dictionary expansion have leveraged word embedding models to find semantically similar words to pre-selected keywords (Roy et al., 2016; Amsler, 2020; Gharibshah et al., 2022; van Atteveldt et al., 2022; Stoll et al., 2023). While systems based on Large Language Models (LLMs) could also be used for dictionary expansion (Jagerman et al., 2023; Wang et al., 2023; Lei et al., 2024), their application is prohibited in certain domains like hate speech or conspiracy-related communication due to strict moderation filters.

In this work, we build on the idea of data-driven dictionaries but rather than focusing only on words most directly similar to pre-selected keywords, we propose Local Graph-based Dictionary Expansion (LGDE), a method that incorporates tools from manifold learning and network science to explore a graph of semantic similarities built from a domain-specific word level representation. LGDE
expands a pre-selected set of keywords by adding words from their corresponding semantic communities, as determined through fast local community detection in the semantic network (Yu et al., 2020). Through a graph-based manifold representation, LGDE thus captures the local nonlinear geometry of domain-specific word embeddings around seed keywords, and then exploits graph diffusion to find local semantic communities that naturally include multi-step word associations.

To evaluate our method, we consider the task of expanding a dictionary of pre-selected keywords from a human-coded hate speech dataset from the social media platforms Reddit ${ }^{1}$ and $\mathrm{Gab}^{2}$ (Qian et al., 2019). As compared to approaches based on direct word similarities, LGDE leads to a betterexpanded dictionary in terms of $F_{1}$ scores with discovered words significantly more likely to appear in hate speech-related communication. To further showcase the potential of LGDE in a realworld use case, we analyze conspiracy-related posts from the message forum 4 chan $^{3}$ collected through an expert-selected dictionary representative of two conspiracy theories ('Great Replacement' and 'New World Order'). In this case, LGDE shows a quantitative advantage in discovering additional relevant words that would be missed without the chain of word associations

### 1.1 Problem definition

Let us consider a pre-selected list of $n$ keywords $W_{0}=\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$, denoted the seed dictionary, which are known to be relevant to a certain topic. These initial terms are usually derived from expert knowledge, literature research or existing keyword lists (Gharibshah et al., 2022; Heft et al., 2023). Let us further assume that we have access to a domain-specific corpus of $l$ documents $D=$ $\left\{d_{1}, d_{2}, \ldots, d_{l}\right\}$ related to the topic of interest, and each keyword in $W_{0}$ is contained in at least one document.We can then formulate the dictionary expansion problem: expand the seed dictionary $W_{0}$ by $m$ new words from the domain-specific corpus $D$ to obtain a data-driven expanded dictionary $W=\left\{w_{1}, w_{2}, \ldots, w_{n}, w_{n+1}, \ldots, w_{n+m}\right\}$ such that the newly discovered keywords make $W$ 'more representative' of the topics of interest as measured by evaluation metrics such as the $F_{1}$ score that balances precision and recall.[^0]

### 1.2 Related work

Keyword extraction and query expansion are related tasks but not the same as dictionary expansion. The former refers to the extraction of representative keywords from text without an initial seed dictionary (Firoozeh et al., 2020), whereas the latter relies on user input on query words or phrases, possibly expanding the current query term or suggesting additional terms (Schütze et al., 2008). These tasks have been studied mostly in the context of information retrieval in search engines and often involve relevance user feedback on retrieved documents (Zheng et al., 2020). Here, we use semantic relationships captured in the latent representation space and focus on generating relevant keyword or query terms based on curated seed keywords, without explicit user feedback. Early statistical approaches were based on ranking candidate keywords using TF-IDF or TextRank (Mihalcea and Tarau, 2004) or analysing word co-occurrences directly (Yin et al., 2020). Promising work has also leveraged pre-trained word embeddings to expand a seed dictionary by most similar words (Amsler, 2020; Gharibshah et al., 2022; van Atteveldt et al., 2022; Stoll et al., 2023). Prior work also suggests that global word embeddings may underperform in tasks that benefit from local properties (Diaz et al., 2016). Farmanbar et al. (2020) explore domainspecific query terms but focus on important challenges in end-to-end production pipelines using direct cosine similarities. Tsai and Wang (2014) and Gharibshah et al. (2022) use similar methods adapted to custom domains. The latter are close to our domain-specific setting but do not employ the properties of the semantic network.

### 1.3 Motivation

A data-driven augmented dictionary can be constructed by adding words from a domain-specific vocabulary $V$ of word embedding vectors that are most similar to the keywords in the seed dictionary $W_{0} \subseteq V$ according to the cosine similarity $S_{\text {cos }}$, given by $S_{\text {cos }}(\boldsymbol{u}, \boldsymbol{v}):=\frac{<\boldsymbol{u}, \boldsymbol{v}>}{\|\boldsymbol{u}\|_{2}\|\boldsymbol{v}\|_{2}}$ for two words $\boldsymbol{u}, \boldsymbol{v} \in V$. For a threshold $0 \leqslant \epsilon \leqslant 1$, the thresholding-based expanded dictionary $W(\epsilon)$ is defined as

$$
\begin{equation*}
W(\epsilon):=\bigcup_{\boldsymbol{w} \in W_{0}}\left\{\boldsymbol{v} \in V \mid S_{\cos }(\boldsymbol{w}, \boldsymbol{v}) \geqslant \epsilon\right\} \tag{1}
\end{equation*}
$$

Choosing the parameter $\epsilon$ appropriately can enrich the seed dictionary by considering direct word as-
sociations. To further improve the quality of the expanded dictionary, one can fine-tune the word embeddings in $V$ on a domain-specific corpus $D$ to better capture contextual semantics.

An issue not addressed by thresholding approaches is that direct similarities can be uninformative in noisy, latent space representations, such as word embeddings. This can lead to relatively unspecific word associations in text. One way to circumvent this limitation is to construct geometric graphs that capture the local manifold and explore it via diffusion. This allows us to consider chains of word associations as paths on the graph. Indeed, a seed keyword $\boldsymbol{w} \in W_{0}$ could be similar to a word $\boldsymbol{u} \in V$ (with $S_{\mathrm{cos}}(\boldsymbol{w}, \boldsymbol{u})>\epsilon$ ) which is in turn similar to another word $\boldsymbol{v} \in V$ (with $S_{\cos }(\boldsymbol{u}, \boldsymbol{v})>\epsilon$ ), yet we might have low direct similarity $S_{\mathrm{cos}}(\boldsymbol{w}, \boldsymbol{v})<\epsilon$, reminiscent of cosine similarity not fulfilling the standard triangle inequality (Schubert, 2021). A method based only on direct similarities would then exclude the word $\boldsymbol{v}$ from the data-driven dictionary $W(\epsilon)$, although the chain of strong word associations arguably makes $v$ a sensible candidate for dictionary expansion. Similar problems may also occur when adding the $k$ most similar words for each seed keyword. LGDE uses tools from manifold learning and network scienceto account for such chains of word associations to better capture the local nonlinear geometry of seed keywords, (see Figure 1 for an illustration).

![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-03.jpg?height=405&width=791&top_left_y=1725&top_left_x=244)

Figure 1: In contrast to (a) simple thresholding, (b) LGDE captures the nonlinear local geometry of word embeddings around a seed keyword.

## 2 Methodology

LGDE consists of three steps. In the first step, we derive vectors for the words in our corpus using a fine-tuned word embedding model. In the second step, we compute a similarity graph from the word vectors that captures the local semantic sim- ilarities of the corpus. In the third step, we use local community detection based on graph diffusion to obtain semantic communities of each seed keyword as candidates for dictionary expansion. We detail these steps in the following subsections and also describe a validation strategy for our method.

### 2.1 Fine-tuned word representations

As a starting point, we use GloVe as base word embeddings, generated from general-purpose corpora like Wikipedia 2014 and Gigaword 5 (Pennington et al., 2014). The base GloVe embeddings are available in different dimensions $r=$ $50,100,300$. It is well-known that word embeddings are dependent on the corpus they are trained on. Given that terms often adopt new or additional meanings across domains or over time (e.g., "Apple" may refer to a concept in technology or business domains that came into existence only in the 1970s), we use Mittens (Dingwall and Potts, 2018) to fine-tune GloVe representations to better represent the semantic relationships in our use-case domains. For a set of $N$ words in our domain-specific corpus $D$, the word embeddings $\boldsymbol{v}_{i}, i=1, \ldots, N$, are computed from a retrofitting model that optimises the Mittens cost function

$$
\begin{equation*}
J_{\text {Mittens }}=J_{\mathrm{GloVe}}+\mu \sum_{i \in U}\left\|\boldsymbol{v}_{i}-\boldsymbol{u}_{i}\right\|, \tag{2}
\end{equation*}
$$

where $J_{\text {Glove }}$ is the standard GloVe cost function (Pennington et al., 2014) based on the word co-occurrences in $D, U$ is the index set of words for which pre-trained vector representations $\boldsymbol{u}_{i}$ are available, and $0 \leqslant \mu$ is the hyperparameter that determines the extent of fine-tuning. A small $\mu$ favors greater adaptation of the word representations $\boldsymbol{v}_{i}$ whereas a large $\mu$ favors remaining closer to the base embeddings $\boldsymbol{u}_{i}$. Setting $\mu=0$ means that the word vectors are essentially computed from scratch on the the new corpus. Although the value $\mu=0.1$ was the default used by Dingwall and Potts (2018), we find that a larger value of $\mu$ can improve the quality of embeddings for a small domain-specific corpus $D$. By training the Mittens model, we compute fine-tuned $r$-dimensional word vectors $V=\left\{\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \ldots, \boldsymbol{v}_{N}\right\} \subsetneq \mathbb{R}^{r}$, for $r=50,100,300$, and we assume $W_{0} \subsetneq V$. While Schnabel et al. (2015) have shown that the length of the word vectors may encode term frequency, we normalize the word vectors to unit length, as the length carries no semantic meaning in our case.

Regarding our choice of GloVe and Mittens, it is worth noting that BERT-based (Devlin et al., 2019) models rely on subword tokenization and, typically, word embeddings are extracted by summing or averaging the subword token embeddings, a heuristic that often degrades the wordlevel semantic network. For instance, a Semeval task on diachronic semantics (Schlechtweg et al., 2020) showed that static or type-based embeddings outperformed contextualized embeddings such as BERT or ELMo (Peters et al., 2018). It is also worth pointing out that BERT-style models are designed for supervised end-to-end finetuning, and not for extracting intermediate embedding layers. Previous studies on obtaining representations from BERT-like models (Vulić et al., 2020) have shown the various issues with representations produced by averaging hidden layers. These are highly task- and domain-dependent and there are no techniques to select a single layer that is reliably better (Bommasani et al., 2020). Work on hidden layer selection for good word-level representations may be an interesting direction but is not the focus of this work.

### 2.2 Semantic network construction

The analysis of the semantic space $V \subsetneq \mathbb{R}^{r}$ is often restricted to the computation of pairwise cosine similarities between words. To explore the full semantic space of our vocabulary $V$ with size $N=|V|$, we construct an undirected, weighted semantic similarity graph $G=(V, E)$, where the nodes correspond to the words in $V$. The weighted edges $E$ are computed from the $N \times N$ matrix of normalised cosine distances:

$$
\begin{equation*}
\tau:=\left\|1-S_{\cos }\right\|_{\max } \tag{3}
\end{equation*}
$$

where $S_{\mathrm{cos}}$ is the cosine similarity and $\|\cdot\|_{\max }$ is the element-wise normalisation with the maxnorm so that all elements of $\tau$ are normalised between $[0,1]$. We also define the matrix of normalised cosine similarities (Altuncu et al., 2019):

$$
\begin{equation*}
S:=1-\tau \tag{4}
\end{equation*}
$$

We would like to obtain a semantic network with edges weighted by the similarity $S$ but $S$ is a dense matrix that contains many small values corresponding to negligible word similarities. To uncover a robust semantic network that only contains the most relevant chains of semantic similarities, we first obtain the undirected and unweighted
Continuous $k$-Nearest Neighbors (CkNN) graph from the distance matrix $\tau$ (Berry and Sauer, 2019). The $N \times N$ adjacency matrix $B^{(k)}$, of the CkNN graph is given by:

$B_{\boldsymbol{u}, \boldsymbol{v}}^{(k)}:= \begin{cases}1 & \text { if } \tau(\boldsymbol{u}, \boldsymbol{v})<\delta \sqrt{\tau\left(\boldsymbol{u}, \boldsymbol{u}_{k}\right) \tau\left(\boldsymbol{v}, \boldsymbol{v}_{k}\right)} \\ 0 & \text { otherwise }\end{cases}$

where $\boldsymbol{u}_{k}, \boldsymbol{v}_{k} \in V$ denote the $k$-th nearest neighbours of $\boldsymbol{u}, \boldsymbol{v} \in V$, respectively, and $\delta>0$ controls the graph density. In contrast with a $k$ Nearest Neighbors (kNN) graph, which does not account for inhomogeneities in the data as it connects a node to all of its $k$ nearest neighbours, the $\mathrm{CkNN}$ construction corrects for different densities and has been shown to approximate consistently the geometry of complex manifolds embedded in a Euclidean space (Berry and Sauer, 2019). Note that $\tau$ is equivalent to using Euclidean distances of normalised word vectors when $\delta=1$. In that case, and assuming $\|\boldsymbol{u}\|_{2}=\|\boldsymbol{v}\|_{2}=1$, we have:

$$
\begin{aligned}
& \tau(\boldsymbol{u}, \boldsymbol{v})<\sqrt{\tau\left(\boldsymbol{u}, \boldsymbol{u}_{k}\right) \tau\left(\boldsymbol{v}, \boldsymbol{v}_{k}\right)} \\
& \Leftrightarrow\|\boldsymbol{u}-\boldsymbol{v}\|_{2}<\sqrt{\left\|\boldsymbol{u}-\boldsymbol{u}_{k}\right\|_{2}\left\|\boldsymbol{v}-\boldsymbol{v}_{k}\right\|_{2}}
\end{aligned}
$$

which follows directly from:

$$
\begin{aligned}
\|\boldsymbol{u}-\boldsymbol{v}\|_{2}^{2} & =\|\boldsymbol{u}\|_{2}^{2}+\|\boldsymbol{v}\|_{2}^{2}-2\langle\boldsymbol{u}, \boldsymbol{v}\rangle_{2} \\
& =2\left(1-S_{\cos }(\boldsymbol{u}, \boldsymbol{v})\right)=C \tau_{\boldsymbol{u}, \boldsymbol{v}}
\end{aligned}
$$

where $C:=2 \max _{\boldsymbol{u}, \boldsymbol{v} \in V}\left(1-S_{\cos }(\boldsymbol{u}, \boldsymbol{v})\right)$ is a constant. Moreover, empirical studies have shown that CkNN with $\delta=1$ and adequate choice of $k$ outperforms other graph constructions for downstream tasks such as data clustering (Liu and Barahona, 2020) and classification (Qian et al., 2021).

Finally, we can define the weighted semantic similarity network $G^{(k)}=\left(V, E^{(k)}\right)$ with adjacency matrix

$$
\begin{equation*}
A^{(k)}:=S \odot B^{(k)} \tag{6}
\end{equation*}
$$

where $\odot$ denotes the element-wise (Hadamard) product. Therefore, the edge weights of the semantic network $G^{(k)}$ are given by the normalised semantic similarity $S$, and its backbone is the sparse CkNN graph $B^{(k)}$ that preserves the topological and local geometric features of the semantic space $V$.

### 2.3 Semantic community detection

The constructed graph $G^{(k)}$ encodes the semantic information contained in our domain-specific
corpus $D$ at a word-level representation with the inter-word weighted edges $E^{(k)}$ capturing relevant semantic similarities between words. Hence, paths in the graph can be interpreted as chains of word associations. Moreover, the keywords in the seed dictionary $W_{0}$ are nodes in the graph and we can study their context and related words by computing their local semantic community structure. To do so, we use the severability method for fast local community detection (Yu et al., 2020) and determine the semantic community $C(\boldsymbol{w})$ for each seed keyword $\boldsymbol{w} \in W_{0}$. Severability is a community detection algorithm that detects local graph communities by exploiting a discrete-time random walk with transition probability matrix $P$, where $P_{u, v}$ is the probability of the random walk jumping from word $\boldsymbol{u} \in V$ to $\boldsymbol{v} \in V$ given by:

$$
\begin{equation*}
0 \leqslant P_{\boldsymbol{u}, \boldsymbol{v}}:=\frac{A_{\boldsymbol{u}, \boldsymbol{v}}^{(k)}}{\sum_{\boldsymbol{v}^{\prime} \in V} A_{\boldsymbol{u}, \boldsymbol{v}^{\prime}}^{(k)}} \leqslant 1 \tag{7}
\end{equation*}
$$

The semantic community $C(\boldsymbol{w})$ of $\boldsymbol{w} \in W_{0}$ is then the subset $C \subseteq V$ (with $\boldsymbol{w} \in C$ ) that maximises the severability quality function $\sigma(C, t)$ for the time scale $t$ :

$$
\begin{equation*}
0 \leqslant \sigma(C, t):=\frac{\rho(C, t)+\mu(C, t)}{2} \leqslant 1 \tag{8}
\end{equation*}
$$

where the mixing term $0 \leqslant \mu(C, t) \leqslant 1$ measures the mixing of the random walker within $C$ over time $t$ and the retention term $0 \leqslant \rho(C, t) \leqslant 1$ quantifies the probability of the random walker not escaping $C$ by time $t$ (see Yu et al. (2020) for details). As $t$ increases, we need a larger-sized community $C \subseteq V$ to trap the random walk and achieve high retention $\rho(C, t)$, but simultaneously, increasing the size of $C$ makes mixing more difficult and leads to a reduced $\mu(C, t)$. To indicate the dependency of the semantic community on $t$ and the $\mathrm{CkNN}$ parameter $k$ we write $C^{(k, t)}(\boldsymbol{w})$. Importantly, severability captures chains of word associations (through paths in the random walk) and allows for overlapping semantic communities (potentially capturing polysemy, if present).

The result of the LGDE method is then the extended dictionary $W(k, t)$ defined as the union of (overlapping) local semantic communities:

$$
\begin{equation*}
W(k, t):=\bigcup_{\boldsymbol{w} \in W_{0}} C^{(k, t)}(\boldsymbol{w}) \tag{9}
\end{equation*}
$$

By construction $W_{0} \subseteq W(k, t) \subseteq V$, and we can expect that the size of $W(k, t)$ generally grows with increasing $k, t \in \mathbb{N}$ such that in the settheoretic limit, we have $\lim _{k, t \rightarrow \infty} W(k, t)=V$. Importantly, our extended dictionary can include words that are connected to a seed keyword $\boldsymbol{w} \in$ $W_{0}$ via a chain of strong word associations.

### 2.4 Evaluation of expanded dictionaries

Consider a domain-specific corpus of $l$ documents $D=\left\{d_{1}, \ldots, d_{l}\right\}$ with ground-truth (humancoded) labels $\boldsymbol{y}$ such that $\boldsymbol{y}_{i}=1$ if document $d_{i}$ has a certain topic of interest (true) and $\boldsymbol{y}_{i}=$ 0 otherwise (false). To assess the quality of a data-driven dictionary $W$ we can evaluate the performance of a simple document classifier $f_{W}$ : $D \rightarrow\{0,1\}$ associated with $W$, where we define $f_{W}(d)=1$ if there exists a keyword in $W$ that appears in the document $d \in D$ and $f_{W}(d)=0$ otherwise. To evaluate the tradeoff between precision and recall of the dictionary $W$ on the potentially unbalanced benchmark data $(D, \boldsymbol{y})$, we compute the macro $F_{1}$ score of its associated classifier $f_{W}$ and denote this number by $F_{1}[W]$. Similarly, $P[W]$ denotes the macro precision and $R[W]$ the macro recall. This evaluation strategy can also be used to train the hyperparameters $0 \leqslant \epsilon \leqslant 1$ in the case of the dictionary $W(\epsilon)$ from Eq. (1) or $k, t$ in the case of the LGDE dictionary $W(k, t)$ from Eq. (9) on a train split of the benchmark data.

It is also possible to evaluate the contribution of a single keyword $w \in W$ to the performance of the dictionary $W$. Let us consider the probability $\mathbb{P}\left[w \in d_{i} \mid y_{i}=1\right]$ that the word $\boldsymbol{w}$ appears in a true document and $\mathbb{P}\left[w \in d_{i} \mid y_{i}=0\right]$ that it appears in a false document. Then we can define the likelihood ratio (LR) (van der Helm and Hische, 1979) of word $\boldsymbol{w}$ as

$$
\begin{equation*}
\operatorname{LR}(\boldsymbol{w}):=\frac{\mathbb{P}\left[w \in d_{i} \mid y_{i}=1\right]}{\mathbb{P}\left[w \in d_{i} \mid y_{i}=0\right]} \tag{10}
\end{equation*}
$$

which is larger than 1 if $\boldsymbol{w}$ is more likely to appear in true documents than in false ones. Words with larger LR can thus be considered to be more representative of true documents. The median LR for all words $\boldsymbol{w} \in W$ denoted by $\operatorname{LR}[W]$ can be used to summarise the individual contributions of keywords to the performance of the dictionary.

## 3 Experiments

### 3.1 Hate speech data

We use a benchmark dataset of 56,099 manually annotated hate speech posts $(D, \boldsymbol{y})$ collected

![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-06.jpg?height=311&width=1585&top_left_y=204&top_left_x=241)

Figure 2: Train $F_{1}$ scores of optimal LGDE dictionaries are higher than of the thresholding-based approach across dictionary sizes in the hate speech experiment for $r=300$. Solid lines are $F_{1}$ scores max-pooled over a window of size 15 , and the dotted line is the train $F_{1}$ score of the seed dictionary for comparison. We observe similar trends for $r=50$ and 100, see Figure 4 in the Appendix.

|  | Seed dictionary | LGDE | Thresholding |
| :--- | :---: | :---: | :---: |
| 50 | 0.856 | $\mathbf{0 . 8 6 1}$ | 0.852 |
| 100 | 0.856 | $\mathbf{0 . 8 7 4}$ | 0.850 |
| 300 | 0.856 | $\mathbf{0 . 8 7 5}$ | 0.846 |

Table 1: Test macro $F_{1}$ scores in hate speech data for different dimensions $r$.

from Reddit and $\mathrm{Gab}^{4}$ by Qian et al. (2019), ${ }^{5}$ of which $19,873(35.4 \%)$ are hate speech ${ }^{6}$ We split the data into train data (75\%) and test data $(25 \%)$ using stratification. We follow Qian et al. (2019) to choose our seed dictionary $W_{0}$ as the five most frequent keywords in the benchmark dataset—"ni**er", "fa**ot", "ret**d", "ret***ed" and " $\mathrm{c} * * \mathrm{t}$ ".

### 3.2 Experimental setup

As outlined in Section 2.1, we first compute finetuned word embeddings $V \subseteq \mathbb{R}^{r}$, filtering out stopwords, infrequent misspellings and very rare words, such that $N:=|V|=7,093$. We found that choosing a relatively large value $\mu=1.0$ produces better-quality word embeddings for various dimensions $r=50,100,300$, where embeddings for out-of-vocabulary (OOV) words are learnt while retaining the quality of pre-existing word embeddings. From the semantic space $V$ we compute the $N \times N$ matrix of normalised cosine similarities $S$ (4).

For each dimension $r$, we then compare the expansion of the seed dictionary $W_{0}$ using the thresholding-based expanded dictionary $W(\epsilon)$[^1]

from Eq. (1) (derived from $S$ ) with the LGDE dictionary $W(k, t)$ from Eq. (9). To control for the size of the different dictionary expansion methods, we require that an expanded dictionary should include at least 30 keywords to facilitate proper comparison but no more than 50 for presentation purposes. Under these constraints, we fine-tune the hyperparameters $0 \leqslant \epsilon \leqslant 1$ of $W(\epsilon)$ and $k, t$ of $W(k, t)$ on the train split of the benchmark data (see Figure 3 and Table 6 in the Appendix for additional information and optimal hyperparameters). We find that LGDE consistently outperforms thresholding across different sizes of the expanded dictionary (Figure 2). We then evaluate the macro F1-scores of the optimised dictionaries on the test split. Finally, we evaluate the individual contributions of keywords in the discovered dictionaries using the likelihood ratio (LR), see Eq. (10), comparing the optimal LGDE dictionary with the thresholding-based dictionary of the closest size.

### 3.3 Results

Table 1 shows that the optimal LGDE dictionaries $W(k, t)$ outperform the optimal thresholdingbased dictionaries $W(\epsilon)$ for all dimensions of the word embeddings, with the overall best dictionary achieved with LGDE at dimension $r=300$. The thresholding-based dictionaries $W(\epsilon)$ do not improve upon the performance of the bare seed dictionary $W_{0}$ because of the inclusion of many nonrelevant terms.

A qualitative assessment of the discovered words (Table 2) shows that LGDE discovers more relevant keywords that lead to hate speech documents, whereas thresholding often produces expected offensive, but largely standard, words without bringing in new semantic context. Some examples of relevant keywords only found by LGDE include "be**er" (a racist term for Hispanic men), " $\mathrm{h} * \mathrm{~g}$ " (a misogynist term for older

| $r$ | LGDE only | Intersection | Thresholding only |
| :---: | :---: | :---: | :---: |
| 50 | altright, awww, baiting, be ${ }^{* *}$, <br> braindead, brainless, btw, commie, <br> commies, $\mathbf{c} * *$ ts, $\mathbf{f} * \mathbf{g}$, fa $* * *$ ts, $\mathbf{f} * *$ ker, <br> gg, go ${ }^{* * \mathbf{m},} \mathbf{h} * \mathbf{g}$, hater, liar, limey, <br> mo*on, pedo, simp, sk**k, s**ts, <br> snowflake, tl, tr**ny, wetback, woah | ![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-07.jpg?height=237&width=355&top_left_y=274&top_left_x=916) | autistic, $\mathbf{b} * * \mathbf{c h}$, bu $* * * *$ it, cared, <br> competent, convicted, $\mathbf{f * * * k}$, in- <br> sane, intellectually, liking, men- <br> tally, morally, offenders, olds, paki, <br> puke, ret $* *$ ds, sane, ugh, yikes |
| 100 | ![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-07.jpg?height=317&width=497&top_left_y=513&top_left_x=396) | $\mathbf{b}^{* *} \mathbf{c h}$, braindead, com- <br> mie, f*g, f**kin, ni ${ }^{* *} \mathbf{a}$, <br> ret**ds, shite, turd, wet- <br> back | as***le, autistic, cared, competent, <br> crybaby, engineers, $\mathbf{f} * * \mathbf{k}, \mathbf{f} * *$ king, <br> hillbilly, honest, incompetent, intel- <br> lectually, mentally, morally, mur- <br> derers, ni ${ }^{* *}$ ers, offenders, slur, <br> weasel, $\mathbf{w}^{* *} \mathbf{r e}$ |
| 300 | ![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-07.jpg?height=350&width=497&top_left_y=829&top_left_x=396) | aww, awww, ni***a, <br> ret $^{* *}$ dation, ret**ds, simp | ![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-07.jpg?height=350&width=468&top_left_y=829&top_left_x=1300) |

Table 2: Discovered words in hate speech data for different embedding dimensions $r$. Words in bold have a large likelihood ratio with $\operatorname{LR}[\boldsymbol{w}] \geqslant 2$.

women), "tr**ny" (a transphobic term for a transgender person) and "go**m" (an antisemitic term for a Jewish person). These derogatory terms, including neologisms and online slang, are part of an informal jargon and are potentially difficult to anticipate beforehand without being part of the online community under investigation. To discover these terms with the thresholding-based approach requires choosing smaller than optimal values of $\epsilon$ and comes with the price of adding many irrelevant terms to $W(\epsilon)$ such that the overall $F_{1}$ performance is reduced. For example, discovering the term "tr**ny" at dimension $r=50$ requires $\epsilon \leqslant 0.719$ such that $|W(\epsilon=0.719)|=192$ with $F_{1}=0.767$; at dimension $r=300$ it requires $\epsilon \leqslant 0.485$ such that $|W(\epsilon=0.485)|=263$ with $F_{1}=0.741$.

| $r$ | LGDE | Thresholding |
| :--- | :---: | :---: |
| 50 | $\mathbf{2 . 2 5}\left(^{*}\right)$ | 1.45 |
| 100 | $\mathbf{2 . 5 9}\left(^{*}\right)$ | 1.53 |
| 300 | $\mathbf{2 . 6 6}\left(^{*}\right)$ | 1.25 |

Table 3: Median likelihood ratios (Eq. 10) of discovered words in hate speech data for different dimensions $r$ with * indicating that LGDE is significantly higher ( $p<0.05$, Mann-Whitney U test).
Table 3 shows that the median LR for words only discovered by LGDE is significantly higher than the median LR for words only discovered by thresholding, i.e, $\operatorname{LR}[W(k, t) \backslash W(\epsilon)]>$ $\operatorname{LR}[W(\epsilon) \backslash W(k, t)]$, and the result is statistically significant for all dimensions ( $p<0.05$, MannWhitney U test). This matches our qualitative assessment that LGDE discovers words more representative of hate speech-related content.

## 4 Application to conspiracy-related content on 4chan

As a further illustration in a real-world use case, we apply LGDE to the problem of collecting conspiracy-related 4chan posts. All content on 4chan is ephemeral and together with complete user anonymity and the absence of content moderation (De Zeeuw and Tuters, 2020) a highly vernacular user culture has developed, which can be partly characterised by its racist and misogynous content. Detecting conspiracy-related content, defined as "the full public communication on and about (alleged) conspiracy theories in various communication venues, including their narrations, counter-narrations, and debunking, as well as neutral observation forms" (Heft et al., 2023, p. 3), can be challenging in this environment as
participants use slang and insider humour to explicitly distinguish themselves from a perceived out-group (De Zeeuw et al., 2020). Therefore, common words used in public debate or scientific literature to describe specific conspiracy theories might deviate from the vocabulary used by 4 chan users. Furthermore, the vocabulary used to describe specific conspiracy theories might change over time, as well as the conspirational narratives themselves, when new events lead to adaptations of conspiracy theories or when new conspiracy theories emerge and are included in the existing canon (Garry et al., 2021; Heft et al., 2023). Using only a literature-based dictionary to retrieve conspiracy-related posts from 4chan is thus insufficient to collect a longitudinal dataset of relevant user comments posted to this platform, rendering a sophisticated method of dictionary expansion necessary. Starting from an expert-selected seed dictionary, we show that LGDE discovers new conspiracy-related words that would be missed without a graph-based perspective.

### 4.1 Data

We assemble an initial seed dictionary $\tilde{W}_{0}$ to be representative of two conspiracy theories ('Great Replacement' and 'New World Order') with 215 keywords including "white genocide", "Illuminati" etc. based on the RPC-Lex dictionary (Puschmann et al., 2022) and other relevant literature (full list in Table 7). Using the fouRplebsAPI (Buehling, 2022), we collect all English language posts published in 22 sample weeks ( 2 weeks in each year from 2011 to 2021) on 4chan's political discussion board /pol/ ${ }^{7}$ leading to a corpus $D$ with 102,058 unique documents. Since many conspiracy-related keywords, such as "great replacement", are multiword phrases, we pre-process the input to include hyphenated terms and noun phrases.

For evaluation and to determine optimal hyperparameters for dictionary expansion with thresholding and LGDE we prepared human-coded benchmark data. As training data, we take a sample of 500 documents from $D$, which was labelled according to the majority vote of three independent human coders (trained student assistants), and we find that 65 documents ( $13.0 \%$ ) are conspiracy-related. We also collected test data independent of $D$ by first sampling a large num-[^2]

ber of random posts from 4chan and then oversampling conspiracy-related documents. The test data consists of 225 documents of which 69 are conspiracy-related $(34.5 \%)$ according to the majority vote of the three independent human coders.

### 4.2 Experimental setup

We restrict our analysis to the 5000 most frequent words in $D$ (excluding stop words but including our seed keywords) denoted by $V$. We then compute fine-tuned word embeddings $V \subseteq \mathbb{R}^{100}$ from our domain-specific corpus $D$, starting from pretrained 100-dimensional GloVe base embeddings, and we use the default value $\mu=0.1$ as our corpus is reasonably large (Dingwall and Potts, 2018). Furthermore, we define the effective seed dictionary $W_{0}=\tilde{W}_{0} \cap V$ as the 109 seed keywords that actually appear in the corpus.

Next, we compare the expansion of the seed dictionary $W_{0}$ using the thresholding-based expanded dictionary $W(\epsilon)$ to the LGDE dictionary $W(k, t)$ both with a maximum of 150 discovered keywords. We perform hyperparameter tuning to obtain the optimal dictionaries but only evaluate the performance of the discovered words $W(\epsilon) \backslash W_{0}$ and $W(k, t) \backslash W_{0}$, as our human-coded train data was collected using the seed dictionary $W_{0}$ (see Table 6 in the Appendix for optimal hyperparameters). To assess the discovered words, three domain experts independently carried out blind annotation of whether discovered terms obtained by both methods (ordered randomly) are suitable to be used as a keyword to search for conspiracyrelated content on 4chan as defined above.

### 4.3 Results

We first evaluate the performance of the two dictionaries using our human-coded test data and we find that the LGDE dictionary $W(k, t)$ has the highest macro $F_{1}$-Score of 0.629 , which is achieved with both higher macro precision and recall than the thresholding-based dictionary $W(\epsilon)$ (Table 5). As expected, the expert-selected seed dictionary has the highest precision but LGDE improves the $F_{1}$ score significantly, in contrast to thresholding.

Our evaluation of the discovered words shows that LGDE discovers significantly more conspiracy-related keywords according to the majority vote of three independent domain experts ( $p<0.005$, Fisher's exact test). In particular, $30.2 \%$ of the words discovered by LGDE are

![](https://cdn.mathpix.com/cropped/2024_06_04_1ad3bf77941f26242fa0g-09.jpg?height=962&width=1562&top_left_y=204&top_left_x=247)

Table 4: Discovered words in conspiracy-related data. Words in bold are classified as conspiracy-related by the majority vote of three independent domain experts $(30.2 \%$ of the words discovered by LGDE and $18.9 \%$ of the words discovered by thresholding). LGDE discovered significantly more conspiracy-related words than thresholdig ( $p<0.005$, Fisher's exact test).

|  | $P$ | $R$ | $F_{1}$ |
| :--- | :---: | :---: | :---: |
| Seed dictionary | $\mathbf{0 . 7 6 9}$ | 0.559 | 0.529 |
| Thresholding | 0.595 | 0.609 | 0.558 |
| LGDE | 0.700 | $\mathbf{0 . 6 2 1}$ | $\mathbf{0 . 6 2 9}$ |

Table 5: Test macro scores for seed and $F_{1}$-optimised discovered dictionaries in application to conspiracyrelated content.

found to be conspiracy-related in contrast to only $18.9 \%$ of the words discovered by thresholding (Table 4). A qualitative assessment of the terms discovered by both methods shows that the quantitative improvement coincides with diverging semantic content of the discovered words. As in the literature-based seed dictionary, many of the terms discovered via thresholding are formal words relating to parts of the population, political philosophies, individuals, or entities. The words discovered via LGDE, on the other hand, are more closely associated with 4chan users' platform-specific rhetoric. They include the anti-Semitic jargon that might seem unremarkable in other contexts, such as "golem" or "good goy" and key vocabulary of related conspiracy narratives, for example "globoho*o", "predictive programming" or "MKUltra". LGDE results also include more multi-word phrases useful for the identification of conspiracy-related posts, such as "Jewish plan" or "Israeli puppet", whose individual components would have been less indicative of conspiracy-related content.

## 5 Conclusion and Discussion

In this work, we have proposed the LGDE framework for data-driven discovery of new keywords that are semantically similar to a pre-defined seed dictionary. Using tools from manifold learning and network science allows us to capture the complex nonlinear geometry of word embeddings and to not only find most similar words but also chains of word associations.

On the task of expanding a seed dictionary of the most frequent hate speech-related keywords, we found that LGDE performs significantly better than a simple thresholding-based approach. In particular, LGDE can take advantage of higher dimensional word embeddings (with richer information in principle) as indicated by higher $F_{1}$-scores and likelihood ratios for $r=300$-dimensional word embeddings. In contrast, the thresholding-
based approach performs worse as the dimensionality is increased. This suggests that LGDE as a manifold learning method better captures the complex nonlinear geometry of high dimensional word embedding spaces, whereas a thresholding-based approach suffers more from the curse of dimensionality. Moreover, in a real-world data collection use case from communication science on a corpus of conspiracy-related 4chan posts, LGDE outperforms thresholding in expanding a curated list of conspiracy-related keywords by platform-specific keywords. Across tasks, the terms provided by LGDE contained a larger variety of formal and informal language, resulting in a heterogeneous set of keywords that represent the neologisms and informal register specific to the corpora under study. This makes LGDE especially informative in cases where researchers cannot assume a comprehensive knowledge of the lexical variety of the object of study.

Studies suggest that the space of word embeddings is at least close to a manifold, e.g., a 'pinched' manifold (Jakubowski et al., 2020). The construction of a $\mathrm{CkNN}$ graph from the word vector embeddings can capture the geometry of complex non-linear manifolds in a manner that is consistent with the geometry of the original space. In particular, Berry and Sauer (2019) show that the $\mathrm{CkNN}$ graph is consistent in the sense that its unnormalised graph Laplacian, which determines the properties of graph diffusions, converges to the Laplace-Beltrami operator in the limit of large data. This preservation of diffusion properties further justifies the subsequent use of the severability method for local community detection (Yu et al., 2020), which is also based on graph diffusion.

## 6 Limitations and future work

We list some limitations in the current work on which we would like to expand in future research. While the experiments employ only English language data, our method is general and the application could be useful for similar data in other languages. It would also be interesting to see the adaptation and evolution of terminology in other domains, e.g., in the scientific literature in relation with the emergence of new sub-disciplines or research areas. Although qualitative assessment is invaluable, the process of manual annotation can be slow and costly and, in particular, labelling hate speech- or conspiracy-related content can pose severe mental health risks to human annotators. In future work, we would like to use LGDE as part of mixed methods approaches (Puschmann et al., 2022) applied to other specialised domains. Specifically, it would be interesting to evaluate the applicability of LGDE to specialised word disambiguation tasks since we observe preliminary evidence of polysemy being captured through overlapping semantic communities (Yu et al., 2020).

## Data and code availability

The hate speech data is publicly available at http s://github.com/jing-qian/A-Bench mark-Dataset-for-Learning-to-Int ervene-in-Online-Hate-Speech and the 4 chan data may be made available upon request. Code for local community detection with severability is available at https://github .com/barahona-research-group/sev erability. An implementation of LGDE and code for all the results and figures presented in this study is available at https://github.com/b arahona-research-group/LGDE.

## Acknowledgements

This study has benefited from discussions in the Advancing Cross-Platform Research in Political Social Media Communication workshop at the Weizenbaum Institute in 2022 and from the first author's presentation in the Computational Methods Division at the International Communication Association Conference in 2023. We thank our student assistants Joana Becker, Dominik Hokamp, Angelika Juhászfor and Katharina Sawade for their invaluable work of labelling benchmark data of conspiracy-related 4chan posts. We also thank Yun William Yu and Angad Khurana for helpful discussions on the implementation of severability and Asem Alaa for valuable suggestions on the Python packaging of the code for this project.

## Funding

Mauricio Barahona acknowledges support from EPSRC grant EP/N014529/1 supporting the EPSRC Centre for Mathematics of Precision Healthcare. Dominik Schindler acknowledges support from the EPSRC (PhD studentship through the Department of Mathematics at Imperial College London) and the Weizenbaum Institute (Research Fellowship). Annett Heft, Kilian Buehling and

Xixuan Zhang acknowledge support by grants from the German Federal Ministry of Education and Research (grant numbers 13N16049 [in the context of the call for proposals Civil Security Societies in Transition] and 16DII135 [in the context of the Weizenbaum Institute]).

## References

M. Tarik Altuncu, Erik Mayer, Sophia N. Yaliraki, and Mauricio Barahona. 2019. From free text to clusters of content in health records: An unsupervised graph partitioning approach. Applied Network Science, 4(1):23.

Michael Amsler. 2020. Using Lexical-Semantic Concepts for Fine-Grained Classification in the Embedding Space. Ph.D. thesis, University of Zurich.

Wouter van Atteveldt, Dafne van Kuppevelt, and Kasper Welbers. 2022. CAVA: An open source $\mathrm{R}$ toolkit for dictionary Coherence, Adaptation, Validation, and Analysis.

Tyrus Berry and Timothy Sauer. 2019. Consistent manifold representation for topological data analysis. Foundations of Data Science, $1(1): 1-38$.

Rishi Bommasani, Kelly Davis, and Claire Cardie. 2020. Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4758-4781, Online. Association for Computational Linguistics.

Axel Bruns, Stephen Harrington, and Edward Hurcombe. 2020. 'Corona? 5G? or both?': The dynamics of COVID-19/5G conspiracy theories on Facebook. Media International Australia, 177(1):12-29.

Kilian Buehling. 2022. fouRplebsAPI: R package for accessing 4 chan posts via the 4 plebs.org API.

Daniël De Zeeuw, Sal Hagen, Stijn Peeters, and Emilija Jokubauskaite. 2020. Tracing normiefication: A cross-platform analysis of the QAnon conspiracy theory. First Monday.
Daniël De Zeeuw and Marc Tuters. 2020. Teh Internet Is Serious Business. Cultural Politics, $16(2): 214-232$.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query expansion with locallytrained word embeddings. arXiv preprint arXiv:1605.07891.

Nicholas Dingwall and Christopher Potts. 2018. Mittens: An Extension of GloVe for Learning Domain-Specialized Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 212217, New Orleans, Louisiana. Association for Computational Linguistics.

Mojtaba Farmanbar, Nikki Van Ommeren, and Boyang Zhao. 2020. Semantic search with domain-specific word-embedding and production monitoring in fintech. In Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations, pages 28-33.

Nazanin Firoozeh, Adeline Nazarenko, Fabrice Alizon, and Béatrice Daille. 2020. Keyword extraction: Issues and methods. Natural Language Engineering, 26(3):259-291.

Amanda Garry, Samantha Walther, Rukaya Rukaya, and Ayan Mohammed. 2021. QAnon Conspiracy Theory: Examining its Evolution and Mechanisms of Radicalization. Journal for Deradicalization, (26):152-216.

Joobin Gharibshah, Jakapun Tachaiya, Arman Irani, Evangelos E. Papalexakis, and Michalis Faloutsos. 2022. IKEA: Unsupervised domain-specific keyword-expansion. In 2022 IEEE/ACM International Conference on

Advances in Social Networks Analysis and Mining (ASONAM), pages 496-503.

Annett Heft, Kilian Buehling, Xixuan Zhang, Dominik J. Schindler, and Miriam Milzner. 2023. Challenges of and approaches to data collection across platforms and time: Conspiracyrelated digital traces as examples of political contention. Journal of Information Technology \& Politics, $0(0): 1-17$.

Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query Expansion by Prompting Large Language Models.

Alexander Jakubowski, Milica Gasic, and Marcus Zibrowius. 2020. Topology of Word Embeddings: Singularities Reflect Polysemy. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pages 103113, Barcelona, Spain (Online). Association for Computational Linguistics.

Ulrike Klinger, W. Lance Bennett, Curd Benjamin Knüpfer, Franziska Martini, and Xixuan Zhang. 2022. From the fringes into mainstream politics: Intermediary networks and movementparty coordination of a global anti-immigration campaign in Germany. Information, Communication \& Society, $0(0): 1-18$.

Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. Query Expansion Using Word Embeddings. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16, pages 1929-1932, New York, NY, USA. Association for Computing Machinery.

Yongho Lee, So Young Kim, Inseok Song, Yongtae Park, and Juneseuk Shin. 2014. Technology opportunity identification customized to the technological capability of SMEs through two-stage patent analysis. Scientometrics, $100(1): 227-244$.

Yibin Lei, Yu Cao, Tianyi Zhou, Tao Shen, and Andrew Yates. 2024. Corpus-Steered Query Expansion with Large Language Models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 393-401, St. Julian's, Malta. Association for Computational Linguistics.
Zijing Liu and Mauricio Barahona. 2020. Graphbased data clustering via multiscale community detection. Applied Network Science, 5(1):3.

Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404-411, Barcelona, Spain. Association for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, Doha, Qatar. Association for Computational Linguistics.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations.

Cornelius Puschmann, Hevin Karakurt, Carolin Amlinger, Nicola Gess, and Oliver Nachtwey. 2022. RPC-Lex: A dictionary to measure German right-wing populist conspiracy discourse online. Convergence, 28(4):1144-1171.

Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, and William Yang Wang. 2019. A Benchmark Dataset for Learning to Intervene in Online Hate Speech. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4755-4764, Hong Kong, China. Association for Computational Linguistics.

Yifan Qian, Paul Expert, Pietro Panzarasa, and Mauricio Barahona. 2021. Geometric graphs from data to aid classification tasks with Graph Convolutional Networks. Patterns, 2(4).

Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, and Utpal Garain. 2016. Using Word Embeddings for Automatic Query Expansion.

Dominik Schlechtweg, Barbara McGillivray, Simon Hengchen, Haim Dubossarsky, and Nina Tahmasebi. 2020. Semeval-2020 task 1: Unsupervised lexical semantic change detection. arXiv preprint arXiv:2007.11464.

Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. 2015. Evaluation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 298-307.

Erich Schubert. 2021. A Triangle Inequality for Cosine Similarity. In Similarity Search and Applications, Lecture Notes in Computer Science, pages 32-44, Cham. Springer International Publishing.

Hinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. 2008. Introduction to information retrieval, volume 39. Cambridge University Press Cambridge.

Anke Stoll, Lena Wilms, and Marc Ziegele. 2023. Developing an Incivility Dictionary for German Online Discussions - a Semi-Automated Approach Combining Human and Artificial Knowledge. Communication Methods and Measures, $0(0): 1-19$.

Ming-Feng Tsai and Chuan-Ju Wang. 2014. Financial keyword expansion via continuous word vector representations. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages $1453-1458$.

H J van der Helm and E A Hische. 1979. Application of Bayes's theorem to results of quantitative clinical chemical determinations. Clinical Chemistry, 25(6):985-988.

Ivan Vulić, Edoardo Maria Ponti, Robert Litschko, Goran Glavaš, and Anna Korhonen. 2020. Probing pretrained language models for lexical semantics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7222-7240, Online. Association for Computational Linguistics.

Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models.

Xicheng Yin, Hongwei Wang, Pei Yin, Hengmin Zhu, and Zhenyu Zhang. 2020. A cooccurrence based approach of automatic keyword expansion using mass diffusion. Scientometrics, 124(3):1885-1905.
Yun William Yu, Jean-Charles Delvenne, Sophia N. Yaliraki, and Mauricio Barahona. 2020. Severability of mesoscale components and local time scales in dynamical networks. (arXiv: 2006.02972).

Jing Zeng and Mike S. Schäfer. 2021. Conceptualizing "Dark Platforms". Covid-19-Related Conspiracy Theories on $8 \mathrm{kun}$ and Gab. Digital Journalism, 9(9):1321-1343.

Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. Bert-qe: contextualized query expansion for document reranking. arXiv preprint arXiv:2009.07258.
</end of paper 2>


